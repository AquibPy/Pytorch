{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import tarfile\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: .\\cifar10.tgz\n"
     ]
    }
   ],
   "source": [
    "# Download the Dataset\n",
    "dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\"\n",
    "download_url(dataset_url,root='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open('./cifar10.tgz', 'r:gz') as tar:\n",
    "    tar.extractall(path='./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'train']\n",
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data/cifar10'\n",
    "print(os.listdir(data_dir))\n",
    "classes = os.listdir(data_dir + '/train')\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above directory structure (one folder per class) is used by many computer vision datasets, and most deep learning libraries provide utilites for working with such datasets. We can use the ImageFolder class from torchvision to load the data as PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(data_dir + '/train', transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a sample element from the training dataset. Each element is a tuple, containing a image tensor and a label. Since the data consists of 32x32 px color images with 3 channels (RGB), each image tensor has the shape ```(3, 32, 32)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32]) 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7922, 0.7922, 0.8000,  ..., 0.8118, 0.8039, 0.7961],\n",
       "         [0.8078, 0.8078, 0.8118,  ..., 0.8235, 0.8157, 0.8078],\n",
       "         [0.8235, 0.8275, 0.8314,  ..., 0.8392, 0.8314, 0.8235],\n",
       "         ...,\n",
       "         [0.8549, 0.8235, 0.7608,  ..., 0.9529, 0.9569, 0.9529],\n",
       "         [0.8588, 0.8510, 0.8471,  ..., 0.9451, 0.9451, 0.9451],\n",
       "         [0.8510, 0.8471, 0.8510,  ..., 0.9373, 0.9373, 0.9412]],\n",
       "\n",
       "        [[0.8000, 0.8000, 0.8078,  ..., 0.8157, 0.8078, 0.8000],\n",
       "         [0.8157, 0.8157, 0.8196,  ..., 0.8275, 0.8196, 0.8118],\n",
       "         [0.8314, 0.8353, 0.8392,  ..., 0.8392, 0.8353, 0.8275],\n",
       "         ...,\n",
       "         [0.8510, 0.8196, 0.7608,  ..., 0.9490, 0.9490, 0.9529],\n",
       "         [0.8549, 0.8471, 0.8471,  ..., 0.9412, 0.9412, 0.9412],\n",
       "         [0.8471, 0.8431, 0.8471,  ..., 0.9333, 0.9333, 0.9333]],\n",
       "\n",
       "        [[0.7804, 0.7804, 0.7882,  ..., 0.7843, 0.7804, 0.7765],\n",
       "         [0.7961, 0.7961, 0.8000,  ..., 0.8039, 0.7961, 0.7882],\n",
       "         [0.8118, 0.8157, 0.8235,  ..., 0.8235, 0.8157, 0.8078],\n",
       "         ...,\n",
       "         [0.8706, 0.8392, 0.7765,  ..., 0.9686, 0.9686, 0.9686],\n",
       "         [0.8745, 0.8667, 0.8627,  ..., 0.9608, 0.9608, 0.9608],\n",
       "         [0.8667, 0.8627, 0.8667,  ..., 0.9529, 0.9529, 0.9529]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = dataset[0]\n",
    "print(img.shape, label)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of classes is stored in the ```.classes``` property of the dataset. The numeric label for each element corresponds to index of the element's label in the list of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for img in dataset[1]:\n",
    "    plt.imshow(img.permute((1,2,0)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000, 5000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = 5000\n",
    "train_size = len(dataset) - val_size\n",
    "train_ds, val_ds = random_split(dataset,[train_size,val_size])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create data loaders for training and validation, to load the data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "batch_size = 128\n",
    "train_dl = DataLoader(train_ds,batch_size,shuffle=True,num_workers=4,pin_memory=True)\n",
    "val_dl = DataLoader(val_ds,batch_size,shuffle=True,num_workers=4,pin_memory=True)\n",
    "#pin_memory (bool, optional) – If True, the data loader will copy Tensors into CUDA pinned memory before returning them.\n",
    "#num_workers (int, optional) – how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model (Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2D convolution is a fairly simple operation at heart: you start with a kernel, which is simply a small matrix of weights. This kernel “slides” over the 2D input data, performing an elementwise multiplication with the part of the input it is currently on, and then summing up the results into a single output pixel.\n",
    "\n",
    "![conv2d](https://miro.medium.com/max/1070/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif)\n",
    "\n",
    "Let us implement a convolution operation on a 1 channel image with a 3x3 kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kernel(image,kernel):\n",
    "    ri, ci = image.shape # image dimensions\n",
    "    rk, ck = kernel.shape # kernel dimensions\n",
    "    ro, co = ri-rk+1, ci-ck+1 # output dimensions\n",
    "    output = torch.zeros([ro,co])\n",
    "    for i in range(ro):\n",
    "        for j in range(co):\n",
    "            output[i,j] = torch.sum(image[i:i+rk, j:j+ck] * kernel)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12., 17.],\n",
       "        [10., 17., 19.],\n",
       "        [ 9.,  6., 14.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_image = torch.tensor([\n",
    "    [3, 3, 2, 1, 0], \n",
    "    [0, 0, 1, 3, 1], \n",
    "    [3, 1, 2, 2, 3], \n",
    "    [2, 0, 0, 2, 2], \n",
    "    [2, 0, 0, 0, 1]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "sample_kernel = torch.tensor([\n",
    "    [0, 1, 2], \n",
    "    [2, 2, 0], \n",
    "    [0, 1, 2]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "apply_kernel(sample_image,sample_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multi-channel images, a different kernel is applied to each channels, and the outputs are added together pixel-wise.\n",
    "\n",
    "There are certain advantages offered by convolutional layers when working with image data:\n",
    "\n",
    "* Fewer parameters: A small set of parameters (the kernel) is used to calculate outputs of the entire image, so the model has much fewer parameters compared to a fully connected layer.\n",
    "\n",
    "* Sparsity of connections: In each layer, each output element only depends on a small number of input elements, which makes the forward and backward passes more efficient.\n",
    "\n",
    "* Parameter sharing and spatial invariance: The features learned by a kernel in one part of the image can be used to detect similar pattern in a different part of another image.\n",
    "\n",
    "We will also use a max-pooling layers to progressively decrease the height & width of the output tensors from each convolutional layer.\n",
    "\n",
    "![max-pooling](https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Conv2d layer transforms a 3-channel image to a 16-channel feature map, and the MaxPool2d layer halves the height and width. The feature map gets smaller as we add more layers, until we are finally left with a small feature map, which can be flattened into a vector. We can then add some fully connected layers at the end to get vector of size 10 for each image.\n",
    "\n",
    "![cnn](https://i.imgur.com/KKtPOKE.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs,labels):\n",
    "    _,preds = torch.max(outputs,dim=1)\n",
    "    return torch.tensor(torch.sum(preds==labels).item()/len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Cifar10CnnModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.network = nn.Sequential(\n",
    "#         nn.Conv2d(3,32,kernel_size=3,padding=1),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(32,64,kernel_size=3,stride=1,padding=1),\n",
    "#         nn.ReLU(),\n",
    "#         nn.MaxPool2d(2,2),# output: 64 x 16 x 16-> using h1=((ho - F +2P)/s) + 1)\n",
    "        \n",
    "#         nn.Conv2d(64,128,kernel_size=3, stride=1,padding=1),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(128,128,kernel_size=3, stride=1,padding=1),\n",
    "#         nn.ReLU(),\n",
    "#         nn.MaxPool2d(2, 2), # output: 128 x 8 x 8-> using h1=((ho - F +2P)/s) + 1)\n",
    "        \n",
    "#         nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.ReLU(),\n",
    "#         nn.MaxPool2d(2, 2),# output: 256 x 4 x 4 -> using h1=((ho - F +2P)/s) + 1)\n",
    "\n",
    "#         nn.Flatten(),\n",
    "#         nn.Linear(256*4*4,1024),# using h1=((ho - F +2P)/s) + 1)\n",
    "#         nn.ReLU(),\n",
    "#         nn.Linear(1024,512),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Linear(512,10))\n",
    "    \n",
    "#     def forward(self,xb):\n",
    "#         return self.network(xb)\n",
    "    \n",
    "#     def training_step(self,batch):\n",
    "#         images,labels = batch\n",
    "#         out = self(images)\n",
    "#         loss = F.cross_entropy(out,labels)\n",
    "#         return loss\n",
    "    \n",
    "#     def validation_step(self,batch):\n",
    "#         images,labels = batch\n",
    "#         out = self(images)\n",
    "#         loss = F.cross_entropy(out,labels)\n",
    "#         acc = accuracy(out,labels)\n",
    "#         return {'val_loss':loss,'val_acc':acc}\n",
    "    \n",
    "#     def validation_epoch_end(self,outputs):\n",
    "#         batch_losses = [x['val_loss'] for x in outputs]\n",
    "#         epoch_loss = torch.stack(batch_losses).mean()\n",
    "#         batch_acc = [x['val_acc'] for x in outputs]\n",
    "#         epoch_acc = torch.stack(batch_acc).mean()\n",
    "#         return {'val_loss':epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "#     def epoch_end(self,epoch,result):\n",
    "#         print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch+1, result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}] val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "        \n",
    "class Cifar10CnnModel(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
    "\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(256*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Cifar10CnnModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Cifar10CnnModel(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU()\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Flatten()\n",
       "    (16): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (17): ReLU()\n",
       "    (18): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (19): ReLU()\n",
       "    (20): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data,device):\n",
    "    if isinstance(data, (list,tuple)): #The isinstance() function returns True if the specified object is of the specified type, otherwise False.\n",
    "        return [to_device(x,device) for x in data]\n",
    "    return data.to(device,non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "val_dl = DeviceDataLoader(val_dl, device)\n",
    "to_device(model, device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "def fit(epochs,lr,model,train_loder,val_loader,opt_func = torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(),lr)\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_loder:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()#used to update the parameters\n",
    "            optimizer.zero_grad()#Clears the gradients of  optimizer\n",
    "        result = evaluate(model,val_loader)\n",
    "        model.epoch_end(epoch,result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = to_device(Cifar10CnnModel(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.3027443885803223, 'val_acc': 0.09843750298023224}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model,val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0] val_loss: 2.3031, val_acc: 0.0984\n",
      "Epoch [1] val_loss: 2.3027, val_acc: 0.1014\n",
      "Epoch [2] val_loss: 2.3032, val_acc: 0.1014\n",
      "Epoch [3] val_loss: 2.3031, val_acc: 0.0984\n",
      "Epoch [4] val_loss: 2.3031, val_acc: 0.0984\n",
      "Epoch [5] val_loss: 2.3027, val_acc: 0.1014\n",
      "Epoch [6] val_loss: 2.3026, val_acc: 0.1043\n",
      "Epoch [7] val_loss: 2.3028, val_acc: 0.1043\n",
      "Epoch [8] val_loss: 2.3029, val_acc: 0.1014\n",
      "Epoch [9] val_loss: 2.3027, val_acc: 0.0984\n"
     ]
    }
   ],
   "source": [
    "history = fit(10,0.001,model,train_dl,val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
